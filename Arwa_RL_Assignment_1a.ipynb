{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArwaSaif99/flutter/blob/master/Arwa_RL_Assignment_1a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Deep Q-Networks (DQN) [Assignment]**\n",
        "\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1sNbFSGUMoRekB-3eZsrjOyK9uo4l7GuG\" alt=\"Robot says hello!\">\n",
        "\n",
        "<hr>\n",
        "\n",
        "### **Part 6**: Reinforcement Learning (from Zero to One)\n",
        "\n",
        "*African Institute for Mathematical Sciences (AIMS), South Africa\n",
        "11 November, 2024*\n",
        "\n",
        "**Arnu Pretorius** - Staff Research Scientist, InstaDeep\n",
        "\n",
        "*Credits*: Adapted from Deep Learning Indaba 2022. Apache License 2.0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "# @title Install required packages (run me) { display-mode: \"form\" }\n",
        "# @markdown This may take a minute or two to complete.\n",
        "%%capture\n",
        "!sudo apt install swig\n",
        "!pip install jaxlib\n",
        "!pip install jax\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install gym\n",
        "!pip install gym[box2d]\n",
        "!pip install optax\n",
        "!pip install matplotlib\n",
        "!pip install chex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gwbqggmcRjMy"
      },
      "outputs": [],
      "source": [
        "# @title Import required packages (run me) { display-mode: \"form\" }\n",
        "%%capture\n",
        "import copy\n",
        "from shutil import rmtree # deleting directories\n",
        "import random\n",
        "import collections # useful data structures\n",
        "import numpy as np\n",
        "import gym # reinforcement learning environments\n",
        "from gym.wrappers import RecordVideo\n",
        "import jax\n",
        "import jax.numpy as jnp # jax numpy\n",
        "import haiku as hk # jax neural network library\n",
        "import optax # jax optimizer library\n",
        "import matplotlib.pyplot as plt # graph plotting library\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import chex\n",
        "\n",
        "# Hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioser_R5TOtp"
      },
      "source": [
        "### **Environment (warm-up)**\n",
        "\n",
        "We will begin by using the simple **CartPole** environment. In CartPole, the task is for the agent to learn to balance a pole for as long as possible by moving a cart *left* or *right*.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/1*v8KcdjfVGf39yvTpXDTCGQ.gif\" width=\"30%\" />\n",
        "\n",
        "- **State space**: The state of the environment is represented by four numbers; *angular position of the pole, angular velocity of the pole, position of the cart, velocity of the cart*.\n",
        "- **Action space**: There are only two actions; *left* and *right*. As such, the actions can be represented by integers $0$ and $1$.  \n",
        "- **Dynamics**: State transitions are deterministic and the agent receives a reward of `1` for every timestep the pole is still upright. If the pole falls over, the game is over and the agent receives no more reward. The game is also over after `500` timesteps, so the maximum reward the agent can collect is `500`.\n",
        "\n",
        "In CartPole, the environment is considered solved when the agent can reliably achieve an episode return of 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJge4jtY9pkz",
        "outputId": "e47e1ac9-3b1c-42ea-a584-2496d5915222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State:: [-0.00586376  1.4067298  -0.5939516  -0.18625809  0.00680142  0.13453904\n",
            "  0.          0.        ]\n",
            "Environment Obs Space Shape: (8,)\n",
            "Environment action space: Discrete(4)\n",
            "Number of actions: 4\n"
          ]
        }
      ],
      "source": [
        "# Create the environment\n",
        "env_name = \"LunarLander-v2\"  # \"LunarLander-v2\" (for later...)\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Reset the environment\n",
        "s_0 = env.reset()\n",
        "print(\"Initial State::\", s_0)\n",
        "\n",
        "# Get environment obs space\n",
        "obs_shape = env.observation_space.shape\n",
        "print(\"Environment Obs Space Shape:\", obs_shape)\n",
        "\n",
        "# Get action space - e.g. discrete or continuous\n",
        "print(f\"Environment action space: {env.action_space}\")\n",
        "\n",
        "# Get num actions\n",
        "num_actions = env.action_space.n\n",
        "print(f\"Number of actions: {num_actions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_HHzFTOc-Qr"
      },
      "source": [
        "## **Q-Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1nOZrUSzhE"
      },
      "source": [
        "\n",
        "In Q-learning the agent learns a function that approximates the **value** of state-action pairs. By *value* we mean the return you expect to receive if you start in a particular state $S_t$, take a particular action $A_t$, and then act according to a particular policy $\\pi$ forever after. The state-action value function of policy $\\pi$ is given by\n",
        "\n",
        "$Q_\\pi(s,a)=\\mathrm{E}_{\\pi}\\left[G_t \\mid S_t=s,\\ A_t=a\\right]$.\n",
        "\n",
        "We say that the value function $Q_\\pi(s,a)$ is the **optimal** value function if the policy $\\pi$ is an optimal policy. We denote the optimal value function as follows:\n",
        "\n",
        "$Q_\\ast(s,a)=\\max \\limits_\\pi \\  \\mathrm{E}_{\\pi}\\left[G_t \\mid S_t=s,\\ A_t=a\\right]$\n",
        "\n",
        "There is an important relationship between the optimal action $a_\\ast$ in a state $s$ and the optimal state-action value function $Q_\\ast$. Namely, the optimal action $a_\\ast$ in state $s$ is equal to the action that maximises the optimal state-action value function. This relationship naturally induces an optimal policy:\n",
        "\n",
        "$\\pi_\\ast(s)=\\arg \\max \\limits_a\\ Q_\\ast(s, a)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2x2tqvZSihz"
      },
      "source": [
        "### **Greedy action selection**\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement a greedy action selector. This should be a function that, given a vector of Q-values, returns the action with the largest Q-value.\n",
        "---\n",
        "\n",
        "**Useful methods:**\n",
        "*   `jnp.argmax` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.argmax.html)) [note from above we had `import jax.numpy as jnp`]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Nn9P1YdzTIDU"
      },
      "outputs": [],
      "source": [
        "# Implement a function takes q-values as input and returns the greedy_action\n",
        "def select_greedy_action(q_values):\n",
        "\n",
        "  # YOUR CODE\n",
        "  action = jnp.argmax(q_values)\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBzLr_G7QKXR",
        "outputId": "6346fe6c-0415-4926-fb22-7a54e3983495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks good!\n"
          ]
        }
      ],
      "source": [
        "# @title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  q_values = jnp.array([1,1,3,4])\n",
        "  action = select_greedy_action(q_values)\n",
        "\n",
        "  if action != 3:\n",
        "    print(\"Oops! It seems your implementation is incorrect.\")\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except Exception as e:\n",
        "  print(\"Oops! It seems you didn't implement anything?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFQUlqZ4ZyLp"
      },
      "source": [
        "### **Q-Network**\n",
        "In Q-learning, we parametrise the Q-function using a neural network $Q_\\phi$. We obtain a policy from the Q-network by always choosing the action with the *greatest* value:\n",
        "\n",
        "$\\hat{\\pi}_\\phi(s)=\\arg \\max \\limits_a\\ Q_{\\phi}(s, a)$\n",
        "\n",
        "We use a neural network to approximate this Q-function. The network will take an observation as input and then output a Q-value for each of the available actions. So in the case of CartPole, the output of the network will have size $2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9wU1soJYZyLp"
      },
      "outputs": [],
      "source": [
        "def build_network(num_actions: int, layers=[64, 64]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for approximating Q-values.\"\"\"\n",
        "\n",
        "  def q_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [hk.Flatten(),\n",
        "         hk.nets.MLP(layers + [num_actions])])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(q_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwG-4qTS1zx"
      },
      "source": [
        "Let's initialise our Q-network and get the initial parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uvq5n2cS9lu",
        "outputId": "00427206-0532-412b-cbbd-9f0d3ab3a896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Learning params: dict_keys(['mlp/~/linear_0', 'mlp/~/linear_1', 'mlp/~/linear_2'])\n"
          ]
        }
      ],
      "source": [
        "# Initialise Q-network\n",
        "Q_NETWORK = build_network(num_actions=num_actions, layers=[20, 20]) # two actions\n",
        "\n",
        "dummy_obs = jnp.zeros((1,*obs_shape), jnp.float32) # a dummy observation like the one in CartPole\n",
        "\n",
        "random_key = jax.random.PRNGKey(42) # random key\n",
        "Q_NETWORK_PARAMS = Q_NETWORK.init(random_key, dummy_obs) # Get initial params\n",
        "\n",
        "print(\"Q-Learning params:\", Q_NETWORK_PARAMS.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbLig3uSZyLp"
      },
      "source": [
        "### **The Bellman Equations**\n",
        "The value function can be written recursively as:\n",
        "\n",
        "$Q_{\\pi}(s, a) = \\mathbb{E}_{s^\\prime \\sim p(s^\\prime |s, a)}\\left[r + \\gamma\\underset{a^{\\prime} \\sim \\pi}{\\mathrm{E}}\\left[Q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\\right]$,\n",
        "\n",
        "Intuitively, this equation says that the value of the action $a$ you took in the state $s$ is equal to the reward $r$ you expect to get, plus the value you expect to get in the next state $s'$ you land in given that you will choose your next action $a'$ with the policy $\\pi$. The Bellman equation for the optimal value function is:\n",
        "\n",
        "$Q_{*}(s, a) = \\mathbb{E}_{s^\\prime \\sim p(s^\\prime |s, a)}\\left[r +\\ \\gamma \\underset{a^{\\prime}}{\\max}\\ Q_{*}(s^{\\prime}, a^{\\prime})\\right]$\n",
        "\n",
        "Notice that instead of chosing your next action $a'$ with policy $\\pi$, we choose the action with the greatest Q-value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsOJi5G8ZyLp"
      },
      "source": [
        "### **The Bellman Backup**\n",
        "To learn to approximate the optimal Q-value function, we can use the right-hand side of the Bellman equation as an update rule. In other words, suppose we have a Q-network $Q_\\phi$, with parameters $\\phi$, then we can iteratively update the parameters such that\n",
        "\n",
        "$Q_\\phi(s,a)\\leftarrow r + \\gamma \\underset{a'}{\\max}\\ Q_\\phi(s', a')$.\n",
        "\n",
        "Intuitively, this says that the approximation of the Q-value of action $a$ in state $s$ should be updated such that it is closer to being equal to the reward received from the environment $r$ plus the value of the best possible action in the next state $s'$. We can perform this optimisation by minimising the difference between the left and right-hand side, with respect to the parameters $\\phi$ using gradient descent. We can measure the difference between the two values using the [squared-error](https://en.wikipedia.org/wiki/Mean_squared_error#Loss_function).\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement the squared-error function.\n",
        "---\n",
        "\n",
        "**Useful functions**\n",
        "* `jnp.square` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.square.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PTto__ohZyLp"
      },
      "outputs": [],
      "source": [
        "def compute_squared_error(pred, target):\n",
        "  # YOUR CODE\n",
        "  squared_error = jnp.square(pred - target)\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGhX8XTFVPVU",
        "outputId": "12b6a8f1-384b-4539-f134-50585cd3fa72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks good!\n"
          ]
        }
      ],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  result = compute_squared_error(1, 4)\n",
        "\n",
        "  if result != 9:\n",
        "    print(\"Oops! It seems your implementation is incorrect.\")\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except Exception as e:\n",
        "  print(\"Oops! It seems you didn't implement anything?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycpZVkgdZyLp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement a function that computes the **Bellman target** (right-hand side of the Bellman equation). If the episode is at the last timestep (i.e. done==1.0), then the Bellman target should be equal to the reward, with no extra value at the end.\n",
        "---\n",
        "\n",
        "**Useful functions**\n",
        "* `jnp.max` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.max.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "961_OllWZyLp"
      },
      "outputs": [],
      "source": [
        "# Bellman target\n",
        "def compute_bellman_target(reward, done, next_q_values, gamma=0.99):\n",
        "  \"\"\"A function to compute the bellman target.\n",
        "\n",
        "  Args:\n",
        "      reward: a scalar reward.\n",
        "      done: a scalar of value either 1.0 or 0.0, indicating if the transition is a terminal one.\n",
        "      next_q_values: a vector of q_values for the next state. One for each action.\n",
        "      gamma: a scalar discount value between 0 and 1.\n",
        "\n",
        "    -- IMPORTANT NOTE: you should keep the default gamma value to check your implementation\n",
        "\n",
        "  Returns:\n",
        "      A scalar equal to the bellman target.\n",
        "\n",
        "  \"\"\"\n",
        "  # YOUR CODE\n",
        "  bellman_target = jnp.where(done, reward, reward + gamma * jnp.max(next_q_values))\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return bellman_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5RAhegOWAkC",
        "outputId": "841d7eda-a15d-456d-98b0-9f16b7f8a323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks good!\n"
          ]
        }
      ],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  # not done\n",
        "  result1 = compute_bellman_target(1, 0.0, np.array([3,2], \"float32\"))\n",
        "\n",
        "  # done\n",
        "  result2 = compute_bellman_target(1, 1.0, np.array([3,2], \"float32\"))\n",
        "\n",
        "  if np.abs(result1 - 3.97) > 0.0001 or result2 != 1:\n",
        "    print(\"Oops! It seems your implementation is incorrect.\")\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except Exception as e:\n",
        "  print(\"Oops! It seems you didn't implement anything?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sIjrHSJZyLq"
      },
      "source": [
        "We can now combine these two functions to compute the loss for Q-learning. The Q-learning loss is equal to the squared difference between the predicted Q-value of an action and its corresponding Bellman target.\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement the Q-learning loss.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LJY_kpFcZyLq"
      },
      "outputs": [],
      "source": [
        "def q_learning_loss(q_values, action, reward, done, next_q_values):\n",
        "    \"\"\"Implementation of the Q-learning loss.T\n",
        "\n",
        "    Args:\n",
        "        q_values: a vector of Q-values, one for each action.\n",
        "        action: an integer, giving the action that was chosen. q_values[action] is the value of the chose action.\n",
        "        done: is a scalar that indicates if this is a terminal transition.\n",
        "        next_q_values: a vector of Q-values in the next state.\n",
        "    Returns:\n",
        "        The squared difference between the q_value of the chosen action and the bellman target.\n",
        "    \"\"\"\n",
        "    # YOUR CODE\n",
        "    chosen_action_q_value = q_values[action]\n",
        "    bellman_target = compute_bellman_target(reward, done, next_q_values)\n",
        "    squared_error = compute_squared_error(chosen_action_q_value, bellman_target)\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aZRn1qaWx2M",
        "outputId": "b03bf57b-a48c-417f-9883-a5131f2b8d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.820902\n",
            "Looks good!\n"
          ]
        }
      ],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  result = q_learning_loss(np.array([3,2], \"float32\"), 1, 2, 0.0, np.array([3,2], \"float32\"))\n",
        "  print(result)\n",
        "\n",
        "  if np.abs(result - 8.820902) > 0.0001:\n",
        "    print(\"Oops! It seems your implementation is incorrect.\")\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except Exception as e:\n",
        "  print(\"Oops! It seems you didn't implement anything?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4YnSUfJZyLq"
      },
      "source": [
        "### **Target Q-network**\n",
        "Notice that when we compute the bellman target we are using our Q-network $Q_\\phi$ to compute the value for the next state $S_{t+1}$. We are basically using our latest approximation of the Q-function to compute the target of our next approximation. Using an approximation to compute the target for your next approximation, is called **bootstrapping**.\n",
        "\n",
        "Unfortunately, if we naively bootstrap like this, it can make training a neural network very unstable. To mitigage this we can instead use a different set of parameters $\\bar{\\phi}$ to compute the values at state $S_{t+1}$. We will keep the parameters $\\bar{\\phi}$ fixed and only periodically update them to be equal to the latest online parameters $\\phi$ every couple of training steps *(say 100)*. This serves to keep the bellman targets fixed for a couple training steps to help reduce the instability due to bootstrapping.\n",
        "\n",
        "\n",
        "We will need to keep track of the latest (online) parameters $\\phi$, as well as the target networks parameters $\\bar{\\phi}$. Lets make a `NamedTuple` to store these two values. We will also need to keep track of the number of learner steps we have taken, so that we know when to update the target network. Lets store a `count` of the learn steps in the `learn_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DvZqUKmq6L7k"
      },
      "outputs": [],
      "source": [
        "# Store online and target parameters\n",
        "QLearnParams = collections.namedtuple(\"Params\", [\"online\", \"target\"])\n",
        "\n",
        "# Q-learn-state\n",
        "QLearnState = collections.namedtuple(\"LearnerState\", [\"count\", \"optim_state\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJWH2_kNZsau"
      },
      "source": [
        "We will be using **Optax** to optimize our neural network. We store the state of the optimizer in the `learn_state` above. Lets now instantiate the optimizer and add the initial Q-network parameters to a `QLearnParams` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HwqKTN6BaAXE"
      },
      "outputs": [],
      "source": [
        "# Initialise Q-network optimizer\n",
        "Q_LEARN_OPTIMIZER = optax.adam(1e-4) # learning rate\n",
        "\n",
        "Q_LEARN_OPTIM_STATE = Q_LEARN_OPTIMIZER.init(Q_NETWORK_PARAMS) # initial optim state\n",
        "\n",
        "# Create Learn State\n",
        "Q_LEARNING_LEARN_STATE = QLearnState(0, Q_LEARN_OPTIM_STATE) # count set to zero initially\n",
        "\n",
        "# Add initial Q-network weights to QLearnParams object\n",
        "Q_LEARNING_PARAMS = QLearnParams(online=Q_NETWORK_PARAMS, target=Q_NETWORK_PARAMS) # target equal to online"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj89M8LgZlFe"
      },
      "source": [
        "Now we can implement a simple function that updates the target network parameters to equal the latest online network parameters every 100 training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SKrg_3rO6cCL"
      },
      "outputs": [],
      "source": [
        "def update_target_params(learn_state, online_weights, target_weights, update_frequency=100):\n",
        "  \"\"\"A function to update target params every 100 training steps\"\"\"\n",
        "\n",
        "  target = jax.lax.cond(\n",
        "      jnp.mod(learn_state.count, update_frequency) == 0,\n",
        "      lambda x, y: x,\n",
        "      lambda x, y: y,\n",
        "      online_weights,\n",
        "      target_weights\n",
        "  )\n",
        "\n",
        "  params = QLearnParams(online_weights, target)\n",
        "\n",
        "  return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoiaYSo9ZyLq"
      },
      "source": [
        "### **Q-learning loss**\n",
        "We now have everything we need to implement the `q_learn` function which takes some batch of transitions and does a step of Q-learning to update the network paramters. But first we use `jax.vmap` to modify the `q_learning_loss` function so that it accepts batches of transitions. In addition, we will compute the Q-values by passing the observations through the `Q_NETWORK` and the target Q-values using the target parameters of the `Q_NETWORK`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hnrsGppWZyLq"
      },
      "outputs": [],
      "source": [
        "def batched_q_learning_loss(online_params, target_params, obs, actions, rewards, next_obs, dones):\n",
        "    q_values = Q_NETWORK.apply(online_params, obs) # use the online parameters\n",
        "    next_q_values = Q_NETWORK.apply(target_params, next_obs) # use the target parameters\n",
        "    squared_error = jax.vmap(q_learning_loss)(q_values, actions, rewards, dones, next_q_values) # vmap q_learning_loss\n",
        "    mean_squared_error = jnp.mean(squared_error) # mean squared error over batch\n",
        "    return mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU8vo9ZebnEa"
      },
      "source": [
        "Now we can create the `q_learn` function which computes the gradient of the `batched_q_learning_loss` and then uses an Optax optimizer to update the network weights and then finally (maybe) updates the target parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6BYoX2W_ZyLr"
      },
      "outputs": [],
      "source": [
        "def q_learn(rng, params, learner_state, memory):\n",
        "  # Compute gradients\n",
        "  grad_loss = jax.grad(batched_q_learning_loss)(params.online, params.target, memory.obs,\n",
        "                                          memory.action, memory.reward,\n",
        "                                          memory.next_obs, memory.done,\n",
        "                                          ) # jax.grad\n",
        "\n",
        "  grad_loss = jax.tree_map(lambda x: jnp.clip(x, -1.0, 1.0), grad_loss)\n",
        "\n",
        "  # grad_loss= jax.tree_map(lambda x: jnp.clip(x ,-1.0 ,1.0),grad_loss)\n",
        "\n",
        "  # Get updates\n",
        "  updates, opt_state = Q_LEARN_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "  # Apply them\n",
        "  new_weights = optax.apply_updates(params.online, updates)\n",
        "\n",
        "  # Maybe update target network\n",
        "  params = update_target_params(learner_state, new_weights, params.target)\n",
        "\n",
        "  # Increment learner step counter\n",
        "  learner_state = QLearnState(learner_state.count + 1, opt_state)\n",
        "\n",
        "  return params, learner_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg9WeeChVYzG"
      },
      "source": [
        "### **A General Purpose RL Training Loop**\n",
        "We have implemented a general purpose RL training loop for you. The training loop takes several arguments as input but the three most important for you to understand are `agent_select_action_func`, `agent_learn_func` and the `agent_memory`.\n",
        "\n",
        "* The `agent_select_action_func` should take an observation and set of `agent_params` as input and should return an action.\n",
        "* The `agent_learn_func` should take the agent's parameters and some \"memories\" as input and then update and return the agents new parameters.\n",
        "* The `agent_memory` is a general purpose module we define that can store some relevant information about the agent's experiences in the environment that can be used in the `agent_learn_func`.\n",
        "\n",
        "Below is the training loop function. You are welcome to go through the code and try to understand it but this is not strictly required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4jra0ykU-GLN"
      },
      "outputs": [],
      "source": [
        "#@title Training loop (run me) { display-mode: \"form\" }\n",
        "\n",
        "# NamedTuple to store transitions\n",
        "Transition = collections.namedtuple(\"Transition\", [\"obs\", \"action\", \"reward\", \"next_obs\", \"done\"])\n",
        "\n",
        "# Training Loop\n",
        "def run_training_loop(env_name, agent_params, agent_select_action_func,\n",
        "    agent_actor_state=None, agent_learn_func=None, agent_learner_state=None,\n",
        "    agent_memory=None, num_episodes=1000, evaluator_period=10,\n",
        "    evaluation_episodes=8, learn_steps_per_episode=1,\n",
        "    train_every_timestep=False, video_subdir=\"\",):\n",
        "    \"\"\"\n",
        "    This function runs several episodes in an environment and periodically does\n",
        "    some agent learning and evaluation.\n",
        "\n",
        "    Args:\n",
        "        env: a gym environment.\n",
        "        agent_params: an object to store parameters that the agent uses.\n",
        "        agent_select_func: a function that does action selection for the agent.\n",
        "        agent_actor_state (optional): an object that stores the internal state\n",
        "            of the agents action selection function.\n",
        "        agent_learn_func (optional): a function that does some learning for the\n",
        "            agent by updating the agent parameters.\n",
        "        agent_learn_state (optional): an object that stores the internal state\n",
        "            of the agent learn function.\n",
        "        agent_memory (optional): an object for storing an retrieving historical\n",
        "            experience.\n",
        "        num_episodes: how many episodes to run.\n",
        "        evaluator_period: how often to run evaluation.\n",
        "        evaluation_episodes: how many evaluation episodes to run.\n",
        "        train_every_timestep: whether to train every timestep rather than at the end\n",
        "            of the episode.\n",
        "        video_subdir: subdirectory to store epsiode recordings.\n",
        "\n",
        "    Returns:\n",
        "        episode_returns: list of all the episode returns.\n",
        "        evaluator_episode_returns: list of all the evaluator episode returns.\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup Cartpole environment and recorder\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\") # training environment\n",
        "    eval_env = gym.make(env_name, render_mode=\"rgb_array\") # evaluation environment\n",
        "\n",
        "    # Video dir\n",
        "    video_dir = \"./video\"+\"/\"+video_subdir\n",
        "\n",
        "    # Clear video dir\n",
        "    try:\n",
        "      rmtree(video_dir)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    # Wrap in recorder\n",
        "    env = RecordVideo(env, video_dir+\"/train\", episode_trigger=lambda x: (x % evaluator_period) == 0)\n",
        "    eval_env = RecordVideo(eval_env, video_dir+\"/eval\", episode_trigger=lambda x: (x % evaluation_episodes) == 0)\n",
        "\n",
        "    # JAX random number generator\n",
        "    rng = hk.PRNGSequence(jax.random.PRNGKey(0))\n",
        "    # env.seed(0) # seed environment for reproducability\n",
        "    random.seed(0)\n",
        "\n",
        "    episode_returns = [] # List to store history of episode returns.\n",
        "    evaluator_episode_returns = [] # List to store history of evaluator returns.\n",
        "    timesteps = 0\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # Reset environment.\n",
        "        obs = env.reset()\n",
        "        episode_return = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # Agent select action.\n",
        "            action, agent_actor_state = agent_select_action_func(\n",
        "                                            next(rng),\n",
        "                                            agent_params,\n",
        "                                            agent_actor_state,\n",
        "                                            np.array(obs)\n",
        "                                        )\n",
        "\n",
        "            # Step environment.\n",
        "            next_obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "            # Pack into transition.\n",
        "            transition = Transition(obs, action, reward, next_obs, done)\n",
        "\n",
        "            # Add transition to memory.\n",
        "            if agent_memory: # check if agent has memory\n",
        "              agent_memory.push(transition)\n",
        "\n",
        "            # Add reward to episode return.\n",
        "            episode_return += reward\n",
        "\n",
        "            # Set obs to next obs before next environment step. CRITICAL!!!\n",
        "            obs = next_obs\n",
        "\n",
        "            # Increment timestep counter\n",
        "            timesteps += 1\n",
        "\n",
        "            # Maybe learn every timestep\n",
        "            if train_every_timestep and (timesteps % 4 == 0) and agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng),\n",
        "                                                        agent_params,\n",
        "                                                        agent_learner_state,\n",
        "                                                        memory\n",
        "                                                    )\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "        # At the end of every episode we do a learn step.\n",
        "        if agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "\n",
        "            for _ in range(learn_steps_per_episode):\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng),\n",
        "                                                        agent_params,\n",
        "                                                        agent_learner_state,\n",
        "                                                        memory\n",
        "                                                    )\n",
        "\n",
        "        if (episode % evaluator_period) == 0: # Do evaluation\n",
        "\n",
        "            evaluator_episode_return = 0\n",
        "            for eval_episode in range(evaluation_episodes):\n",
        "                obs = eval_env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action, _ = agent_select_action_func(\n",
        "                                    next(rng),\n",
        "                                    agent_params,\n",
        "                                    agent_actor_state,\n",
        "                                    np.array(obs),\n",
        "                                    evaluation=True\n",
        "                                )\n",
        "\n",
        "                    obs, reward, done, _ = eval_env.step(int(action))\n",
        "\n",
        "                    evaluator_episode_return += reward\n",
        "\n",
        "            evaluator_episode_return /= evaluation_episodes\n",
        "\n",
        "            evaluator_episode_returns.append(evaluator_episode_return)\n",
        "\n",
        "            logs = [\n",
        "                    f\"Episode: {episode}\",\n",
        "                    f\"Epsilon: {get_epsilon(timesteps)}\",\n",
        "                    f\"Episode Return: {episode_return}\",\n",
        "                    f\"Average Episode Return: {np.mean(episode_returns[-20:])}\",\n",
        "                    f\"Evaluator Episode Return: {evaluator_episode_return}\"\n",
        "            ]\n",
        "\n",
        "            print(*logs, sep=\"\\t\") # Print the logs\n",
        "\n",
        "    env.close()\n",
        "    eval_env.close()\n",
        "\n",
        "    return episode_returns, evaluator_episode_returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpZsKHssZyLq"
      },
      "source": [
        "### **Replay Buffer**\n",
        "For Q-learning, we will need an agent memory that stores entire transitions: `obs`, `action`, `reward`, `next_obs`, `done`. When we retrieve transitions from the memory, they should be chosen randomly from all of the transitions collected so far. In RL, we often call such a module a **replay buffer**. One benefit of using a replay buffer is that experiences can be *re-used* several times for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8tv5dUH6ZyLr"
      },
      "outputs": [],
      "source": [
        "class TransitionMemory(object):\n",
        "  \"\"\"A simple Python replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, max_size=100_000, batch_size=128):\n",
        "    self.batch_size = batch_size\n",
        "    self.buffer = collections.deque(maxlen=max_size)\n",
        "\n",
        "  def push(self, transition):\n",
        "\n",
        "    # add transition to the replay buffer\n",
        "    self.buffer.append(\n",
        "        (transition.obs, transition.action, transition.reward,\n",
        "          transition.next_obs, transition.done)\n",
        "    )\n",
        "\n",
        "\n",
        "  def is_ready(self):\n",
        "    return self.batch_size <= len(self.buffer)\n",
        "\n",
        "  def sample(self):\n",
        "    # Randomly sample a batch of transitions from the buffer\n",
        "    random_replay_sample = random.sample(self.buffer, self.batch_size)\n",
        "\n",
        "    # Batch the transitions together\n",
        "    obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*random_replay_sample)\n",
        "\n",
        "    return Transition(\n",
        "        np.stack(obs_batch).astype(\"float32\"),\n",
        "        np.asarray(action_batch).astype(\"int32\"),\n",
        "        np.asarray(reward_batch).astype(\"float32\"),\n",
        "        np.stack(next_obs_batch).astype(\"float32\"),\n",
        "        np.asarray(done_batch).astype(\"float32\")\n",
        "    )\n",
        "\n",
        "# Instantiate the memory\n",
        "Q_LEARNING_MEMORY = TransitionMemory(max_size=100_000, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbHk03VVUHAV"
      },
      "source": [
        "### **Random exploration**\n",
        "We almost have everything we need for a functioning Q-learning agent. But one problem is that if we always choose the action with the highest Q-value then the agent's policy will be completly *deterministic*. This means the agent will always choose the same strategy. This can pose a problem because at the start of training, the Q-network will be very inaccurate (i.e. a bad aproximation of the true Q-function) and the agent will consistently choose suboptimal actions. Moreover, the agent will never deviate from its suboptimal strategy and discover new, potentially more rewarding  actions. As a result, the Q-network remains inaccurate. Ideally, the agent should try out many different strategies in the beginning so that it can observe the outcomes (rewards) of its actions in different states and so improve its approximation of the Q-function.\n",
        "\n",
        "One easy way to ensure that the agent tries out many different actions is to let it choose some random actions, instead of the greedy (best) action all the time.\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement a random action selector! This should be a function that, given the number of possible (discrete) actions, returns a random action.\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Useful methods:**\n",
        "\n",
        "*  `jax.random.randint` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.randint.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uUKkpMLXUtko"
      },
      "outputs": [],
      "source": [
        "def select_random_action(key, num_actions):\n",
        "\n",
        "    # YOUR CODE\n",
        "    action = jax.random.randint(key,  (1,),0,num_actions)\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO6va6S2Y40E",
        "outputId": "5dfbd5a3-58cd-4de1-eef0-09d2f7198303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "[0]\n",
            "Looks good!\n"
          ]
        }
      ],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  random_key1 = random_key = jax.random.PRNGKey(6) # random key\n",
        "  random_key2 = random_key = jax.random.PRNGKey(1000) # random key\n",
        "  result1 = select_random_action(random_key1, 2)\n",
        "  result2 = select_random_action(random_key2, 2)\n",
        "  print(result1)\n",
        "  print(result2)\n",
        "\n",
        "  if result1 != 1 or result2 != 0:\n",
        "    print(\"Oops! It seems your implementation is incorrect.11\")\n",
        "\n",
        "\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except:\n",
        "  print(\"Oops! It seems your implementation is incorrect.22\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-kKDFT6XU6y"
      },
      "source": [
        "### **$\\varepsilon$-greedy action selection**\n",
        "At the start of training, when the accuracy of the Q-network is low, it is worthwhile for the agent to mostly take random actions so that it can learn about how good or bad certain actions are. However, as the accuracy of the Q-network improves, the agent should start taking fewer random actions and instead start choosing greedy actions with respect to the Q-values. Choosing the best actions given the current Q-network is referred to as **exploitation**. In RL, we typically denote the ratio of random to greedy actions by the value **epsilon** $\\varepsilon$. Epsilon is usually a decimal value in the interval $[0,1]$, where for example $\\varepsilon=0.4$ means that the agent chooses a random action 40% of the time and the greedy action 60% of the time. It is common in RL to linearly (or exponentially) decrease the value of epsilon over time so that the agent becomes increasingly greedy as the accuracy of its Q-network improves through learning.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement an $\\varepsilon$ decay schedule. This function should take the number of timesteps as input and return the current epsilon value.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_qejnbCocurG"
      },
      "outputs": [],
      "source": [
        "EPSILON_DECAY_TIMESTEPS = 10000 # decay epsilon over 3000 timesteps\n",
        "EPSILON_MIN = 0.05 # 10% exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5ujSbssCZyLs"
      },
      "outputs": [],
      "source": [
        "def get_epsilon(num_timesteps):\n",
        "  # YOUR CODE\n",
        "  epsilon = 1-(num_timesteps/EPSILON_DECAY_TIMESTEPS)\n",
        "\n",
        "  epsilon = jax.lax.select(\n",
        "      epsilon < EPSILON_MIN,\n",
        "      EPSILON_MIN, # if less than min then set to min\n",
        "      epsilon # else don't change epsilon\n",
        "  )\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvu4zA64aUou",
        "outputId": "b10e2474-2b7f-4fbd-8c72-72f7fe6777ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oops! It seems your implementation is incorrect.\n"
          ]
        }
      ],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "def check_get_epsilon(get_epsilon):\n",
        "  try:\n",
        "    result1 = get_epsilon(10)\n",
        "    result2 = get_epsilon(5_010)\n",
        "\n",
        "    if result1 != 0.99666667 or result2 != 0.1:\n",
        "      print(\"Oops! It seems your implementation is incorrect.\")\n",
        "    else:\n",
        "      print(\"Looks good!\")\n",
        "  except:\n",
        "    print(\"Oops! It seems your implementation is incorrect.\")\n",
        "\n",
        "check_get_epsilon(get_epsilon)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t56oo58TVQ_s"
      },
      "source": [
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement an $\\varepsilon$-greedy action selector incorporating the schedule from above.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0P2Sa_tqFHZu"
      },
      "outputs": [],
      "source": [
        "def select_epsilon_greedy_action(key, q_values, num_timesteps):\n",
        "    num_actions = len(q_values) # number of available actions\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    epsilon = get_epsilon(num_timesteps)\n",
        "\n",
        "\n",
        "    should_explore = jax.random.uniform(key) < epsilon # hint: a boolean expression to check if some random number is less than epsilon\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        should_explore,\n",
        "        select_random_action(key,num_actions)[0] , # if should explore\n",
        "        jnp.argmax(q_values) # if should be greedy\n",
        "    )\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIffzLgDD_uG",
        "outputId": "6b2833dd-72ae-488e-ff03-46dc859cdf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks good!\n"
          ]
        }
      ],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n",
        "  dummy_q_values = jnp.array([0,1], jnp.float32)\n",
        "  num_timesteps = 5010 # very greedy\n",
        "  actions1 = []\n",
        "  for i in range(10):\n",
        "      actions1.append(int(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps)))\n",
        "\n",
        "  num_timesteps = 0 # completly random\n",
        "  actions2 = []\n",
        "  for i in range(10):\n",
        "      actions2.append(int(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps)))\n",
        "\n",
        "  if actions1 != [1, 1, 0, 1, 1, 0, 1, 1, 1, 1] or actions2 != [0, 0, 0, 1, 1, 1, 1, 0, 0, 0]:\n",
        "    print(\"Oops! It seems your implementation is incorrect.\")\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except:\n",
        "  print(\"Oops! It seems your implementation is incorrect.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5W23MnobN9x"
      },
      "source": [
        "### **Q-learning select action**\n",
        "\n",
        "We now have everything we need to make the `q_learning_select_action` function. We will use the `actor_state` to store a counter which keeps track of the current number of timesteps. We can use the counter to decrement our `epsilon` value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "81TysLc0RjM6"
      },
      "outputs": [],
      "source": [
        "# Actor state stores the current number of timesteps\n",
        "QActorState = collections.namedtuple(\"ActorState\", [\"count\"])\n",
        "\n",
        "def q_learning_select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim\n",
        "    q_values = Q_NETWORK.apply(params.online, obs)[0] # remove batch dim\n",
        "\n",
        "    action = select_epsilon_greedy_action(key, q_values, actor_state.count)\n",
        "    greedy_action = select_greedy_action(q_values)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        evaluation,\n",
        "        greedy_action,\n",
        "        action\n",
        "    )\n",
        "\n",
        "    next_actor_state = QActorState(actor_state.count + 1) # increment timestep counter\n",
        "\n",
        "    return action, next_actor_state\n",
        "\n",
        "Q_LEARNING_ACTOR_STATE = QActorState(0) # counter set to zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z884-1oNRGEr"
      },
      "source": [
        "### **Training**\n",
        "We can now put everything together using the agent-environment loop. We also `jit` the select action function and the learn function for some extra speed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OHBQzg1GcCOs"
      },
      "outputs": [],
      "source": [
        "# Jit functions\n",
        "q_learning_select_action_jit = jax.jit(q_learning_select_action)\n",
        "q_learn_jit = jax.jit(q_learn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbdHDbd1RjM8",
        "outputId": "4f5cc093-48b0-46ba-c0d9-8099e99f903f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training. This may take a few minutes to complete.\n",
            "Episode: 0\tEpsilon: 0.9934999942779541\tEpisode Return: -82.59309168507019\tAverage Episode Return: -82.59309168507019\tEvaluator Episode Return: -649.5162305968427\n",
            "Episode: 10\tEpsilon: 0.8955000042915344\tEpisode Return: -333.7703602413145\tAverage Episode Return: -162.8693358435601\tEvaluator Episode Return: -1199.6105174158483\n",
            "Episode: 20\tEpsilon: 0.7900999784469604\tEpisode Return: -211.1264639137748\tAverage Episode Return: -202.47055519705484\tEvaluator Episode Return: -810.3106769801699\n",
            "Episode: 30\tEpsilon: 0.6854000091552734\tEpisode Return: -253.99227182225698\tAverage Episode Return: -253.60863672002316\tEvaluator Episode Return: -737.399464734431\n",
            "Episode: 40\tEpsilon: 0.5547000169754028\tEpisode Return: -259.79082847322593\tAverage Episode Return: -225.68441038220772\tEvaluator Episode Return: -832.2048722494467\n",
            "Episode: 50\tEpsilon: 0.45419999957084656\tEpisode Return: -178.7513575250351\tAverage Episode Return: -192.8022236205922\tEvaluator Episode Return: -129.82016664143458\n",
            "Episode: 60\tEpsilon: 0.3792000114917755\tEpisode Return: -110.1961481401423\tAverage Episode Return: -172.0287224816995\tEvaluator Episode Return: -112.18625182289153\n",
            "Episode: 70\tEpsilon: 0.2994999885559082\tEpisode Return: -138.0584083247859\tAverage Episode Return: -134.6427385808551\tEvaluator Episode Return: -132.1714412586154\n",
            "Episode: 80\tEpsilon: 0.21969999372959137\tEpisode Return: -160.91940997553618\tAverage Episode Return: -127.97626697883884\tEvaluator Episode Return: -120.56596337973693\n",
            "Episode: 90\tEpsilon: 0.11410000175237656\tEpisode Return: -76.90978096928387\tAverage Episode Return: -177.2578799074789\tEvaluator Episode Return: -214.75232493442576\n",
            "Episode: 100\tEpsilon: 0.05000000074505806\tEpisode Return: -152.6084658938919\tAverage Episode Return: -180.46425189890834\tEvaluator Episode Return: -135.4146705643968\n",
            "Episode: 110\tEpsilon: 0.05000000074505806\tEpisode Return: -146.80734939303585\tAverage Episode Return: -130.277703631698\tEvaluator Episode Return: -123.03958819474848\n",
            "Episode: 120\tEpsilon: 0.05000000074505806\tEpisode Return: -132.49807253446494\tAverage Episode Return: -129.85967701845942\tEvaluator Episode Return: -123.83767854726737\n",
            "Episode: 130\tEpsilon: 0.05000000074505806\tEpisode Return: -155.47824069134128\tAverage Episode Return: -127.91092597586074\tEvaluator Episode Return: -118.9171190253898\n",
            "Episode: 140\tEpsilon: 0.05000000074505806\tEpisode Return: -88.35945154418714\tAverage Episode Return: -101.10187439124492\tEvaluator Episode Return: -9.545834215409798\n",
            "Episode: 150\tEpsilon: 0.05000000074505806\tEpisode Return: -285.8929438952881\tAverage Episode Return: -133.10629433564955\tEvaluator Episode Return: -146.98558911627845\n",
            "Episode: 160\tEpsilon: 0.05000000074505806\tEpisode Return: -188.05253563205753\tAverage Episode Return: -158.33990850325614\tEvaluator Episode Return: -152.09191299330553\n",
            "Episode: 170\tEpsilon: 0.05000000074505806\tEpisode Return: -85.39122771633961\tAverage Episode Return: -121.51157112454136\tEvaluator Episode Return: -55.99948467576793\n",
            "Episode: 180\tEpsilon: 0.05000000074505806\tEpisode Return: -84.9827127340254\tAverage Episode Return: -134.17400253246834\tEvaluator Episode Return: -67.1915730326513\n",
            "Episode: 190\tEpsilon: 0.05000000074505806\tEpisode Return: -164.94602223871695\tAverage Episode Return: -109.13399160722118\tEvaluator Episode Return: -131.57858022974955\n",
            "Episode: 200\tEpsilon: 0.05000000074505806\tEpisode Return: 242.924077023424\tAverage Episode Return: -39.929525984814404\tEvaluator Episode Return: -141.8194564799805\n",
            "Episode: 210\tEpsilon: 0.05000000074505806\tEpisode Return: -277.1055533217906\tAverage Episode Return: -64.06786733133377\tEvaluator Episode Return: -124.5354796190095\n",
            "Episode: 220\tEpsilon: 0.05000000074505806\tEpisode Return: -152.5202146621166\tAverage Episode Return: -81.47797202604683\tEvaluator Episode Return: -165.5356744681933\n",
            "Episode: 230\tEpsilon: 0.05000000074505806\tEpisode Return: -187.37932953562603\tAverage Episode Return: -134.84724835132124\tEvaluator Episode Return: -254.30000324687668\n",
            "Episode: 240\tEpsilon: 0.05000000074505806\tEpisode Return: -107.49476364966443\tAverage Episode Return: -222.79062284263105\tEvaluator Episode Return: -303.53600957832083\n",
            "Episode: 250\tEpsilon: 0.05000000074505806\tEpisode Return: -339.36910356774547\tAverage Episode Return: -254.70025284205494\tEvaluator Episode Return: -408.5793306120409\n",
            "Episode: 260\tEpsilon: 0.05000000074505806\tEpisode Return: -26.635325604421766\tAverage Episode Return: -310.36059768225783\tEvaluator Episode Return: -262.3669596719705\n",
            "Episode: 270\tEpsilon: 0.05000000074505806\tEpisode Return: -576.6240528088044\tAverage Episode Return: -303.2427086958255\tEvaluator Episode Return: -464.6255764004152\n",
            "Episode: 280\tEpsilon: 0.05000000074505806\tEpisode Return: -361.8719812393333\tAverage Episode Return: -324.9040166452029\tEvaluator Episode Return: -546.5494214099284\n",
            "Episode: 290\tEpsilon: 0.05000000074505806\tEpisode Return: -602.366261630874\tAverage Episode Return: -393.04251539824645\tEvaluator Episode Return: -447.25462209130745\n",
            "Episode: 300\tEpsilon: 0.05000000074505806\tEpisode Return: -594.0320681755485\tAverage Episode Return: -455.271635054887\tEvaluator Episode Return: -423.61615795665347\n",
            "Episode: 310\tEpsilon: 0.05000000074505806\tEpisode Return: -640.8465710876582\tAverage Episode Return: -502.95595813337695\tEvaluator Episode Return: -394.7505417993451\n",
            "Episode: 320\tEpsilon: 0.05000000074505806\tEpisode Return: -267.316907604918\tAverage Episode Return: -445.70358521859663\tEvaluator Episode Return: -465.28679245891834\n",
            "Episode: 330\tEpsilon: 0.05000000074505806\tEpisode Return: -502.15940528080574\tAverage Episode Return: -436.66522144655676\tEvaluator Episode Return: -369.6357550658204\n",
            "Episode: 340\tEpsilon: 0.05000000074505806\tEpisode Return: -427.5516620246781\tAverage Episode Return: -428.34686519497484\tEvaluator Episode Return: -319.13641511606795\n",
            "Episode: 350\tEpsilon: 0.05000000074505806\tEpisode Return: -113.18502894732444\tAverage Episode Return: -365.4249880041684\tEvaluator Episode Return: -328.85764260717673\n",
            "Episode: 360\tEpsilon: 0.05000000074505806\tEpisode Return: -455.45950453232643\tAverage Episode Return: -356.8670075752815\tEvaluator Episode Return: -203.21222009555984\n",
            "Episode: 370\tEpsilon: 0.05000000074505806\tEpisode Return: -71.2727030894921\tAverage Episode Return: -326.91473193033846\tEvaluator Episode Return: -130.54159301746765\n",
            "Episode: 380\tEpsilon: 0.05000000074505806\tEpisode Return: -102.52708792743408\tAverage Episode Return: -190.1854402885174\tEvaluator Episode Return: -106.12678661096233\n",
            "Episode: 390\tEpsilon: 0.05000000074505806\tEpisode Return: 38.79222068686943\tAverage Episode Return: -89.70425676707336\tEvaluator Episode Return: -67.59816329896287\n",
            "Episode: 400\tEpsilon: 0.05000000074505806\tEpisode Return: -65.62054509602743\tAverage Episode Return: -53.549843712586345\tEvaluator Episode Return: -35.36043262412423\n",
            "Episode: 410\tEpsilon: 0.05000000074505806\tEpisode Return: -49.15890835177527\tAverage Episode Return: -23.320592867194243\tEvaluator Episode Return: -50.49832102702629\n",
            "Episode: 420\tEpsilon: 0.05000000074505806\tEpisode Return: -94.72868365879266\tAverage Episode Return: -45.31742463974506\tEvaluator Episode Return: -66.59145961652136\n",
            "Episode: 430\tEpsilon: 0.05000000074505806\tEpisode Return: -36.11317240210664\tAverage Episode Return: -61.45789035927861\tEvaluator Episode Return: -46.64652087536838\n",
            "Episode: 440\tEpsilon: 0.05000000074505806\tEpisode Return: -44.449660630272675\tAverage Episode Return: -75.95501256236317\tEvaluator Episode Return: -58.812912718050896\n",
            "Episode: 450\tEpsilon: 0.05000000074505806\tEpisode Return: -122.19935432013774\tAverage Episode Return: -79.77169376449855\tEvaluator Episode Return: -103.65740383523921\n",
            "Episode: 460\tEpsilon: 0.05000000074505806\tEpisode Return: -105.57909649439155\tAverage Episode Return: -93.65712323681639\tEvaluator Episode Return: -80.00777710958133\n",
            "Episode: 470\tEpsilon: 0.05000000074505806\tEpisode Return: -127.95838728138479\tAverage Episode Return: -106.29311197356245\tEvaluator Episode Return: -103.53041248392303\n",
            "Episode: 480\tEpsilon: 0.05000000074505806\tEpisode Return: -100.16948122912694\tAverage Episode Return: -106.18712403668334\tEvaluator Episode Return: -95.31744136450304\n",
            "Episode: 490\tEpsilon: 0.05000000074505806\tEpisode Return: -78.55810998473194\tAverage Episode Return: -97.78382705681501\tEvaluator Episode Return: -91.64016508858518\n",
            "Episode: 500\tEpsilon: 0.05000000074505806\tEpisode Return: -127.76453975116016\tAverage Episode Return: -92.80968089609952\tEvaluator Episode Return: -121.68350414425507\n",
            "Episode: 510\tEpsilon: 0.05000000074505806\tEpisode Return: -103.56478450474798\tAverage Episode Return: -104.9629638392832\tEvaluator Episode Return: -118.63271816738335\n",
            "Episode: 520\tEpsilon: 0.05000000074505806\tEpisode Return: -80.34383249991313\tAverage Episode Return: -99.02287009657428\tEvaluator Episode Return: -70.3538412158172\n",
            "Episode: 530\tEpsilon: 0.05000000074505806\tEpisode Return: -41.491717483041086\tAverage Episode Return: -88.8259224268008\tEvaluator Episode Return: -79.58118924020175\n",
            "Episode: 540\tEpsilon: 0.05000000074505806\tEpisode Return: -126.88720383115944\tAverage Episode Return: -86.64900060828741\tEvaluator Episode Return: -75.89076932817491\n",
            "Episode: 550\tEpsilon: 0.05000000074505806\tEpisode Return: -130.22119348488718\tAverage Episode Return: -89.02640361072882\tEvaluator Episode Return: -97.69717025948025\n",
            "Episode: 560\tEpsilon: 0.05000000074505806\tEpisode Return: -71.93992347436401\tAverage Episode Return: -82.72868580757974\tEvaluator Episode Return: -88.35927768116356\n",
            "Episode: 570\tEpsilon: 0.05000000074505806\tEpisode Return: -67.04687572462785\tAverage Episode Return: -80.76028508860026\tEvaluator Episode Return: -68.96578822947274\n",
            "Episode: 580\tEpsilon: 0.05000000074505806\tEpisode Return: -109.72048197467007\tAverage Episode Return: -92.48124515980079\tEvaluator Episode Return: -101.73657025736597\n",
            "Episode: 590\tEpsilon: 0.05000000074505806\tEpisode Return: -71.49654702280489\tAverage Episode Return: -87.25136671585821\tEvaluator Episode Return: -98.20686073530054\n",
            "Episode: 600\tEpsilon: 0.05000000074505806\tEpisode Return: -131.87805826391767\tAverage Episode Return: -78.40771676029037\tEvaluator Episode Return: -125.73027148533491\n",
            "Episode: 610\tEpsilon: 0.05000000074505806\tEpisode Return: -118.63362523311115\tAverage Episode Return: -89.79439267513808\tEvaluator Episode Return: -150.69358675671356\n",
            "Episode: 620\tEpsilon: 0.05000000074505806\tEpisode Return: -131.64737644370996\tAverage Episode Return: -119.67942118787812\tEvaluator Episode Return: -146.8523870911592\n",
            "Episode: 630\tEpsilon: 0.05000000074505806\tEpisode Return: -144.3220598501829\tAverage Episode Return: -129.59045809771888\tEvaluator Episode Return: -175.13915870656476\n",
            "Episode: 640\tEpsilon: 0.05000000074505806\tEpisode Return: -117.06498158057633\tAverage Episode Return: -132.94466010122784\tEvaluator Episode Return: -161.4382751300714\n",
            "Episode: 650\tEpsilon: 0.05000000074505806\tEpisode Return: -107.80121360124586\tAverage Episode Return: -145.8294810035319\tEvaluator Episode Return: -137.1873981225877\n",
            "Episode: 660\tEpsilon: 0.05000000074505806\tEpisode Return: -99.80553995021913\tAverage Episode Return: -146.95501343774993\tEvaluator Episode Return: -144.61080563085054\n",
            "Episode: 670\tEpsilon: 0.05000000074505806\tEpisode Return: -113.30149281422581\tAverage Episode Return: -146.89881167941405\tEvaluator Episode Return: -82.96000590877465\n",
            "Episode: 680\tEpsilon: 0.05000000074505806\tEpisode Return: -144.19748881655363\tAverage Episode Return: -147.62106766604043\tEvaluator Episode Return: -159.49521896803685\n",
            "Episode: 690\tEpsilon: 0.05000000074505806\tEpisode Return: -184.2877443775078\tAverage Episode Return: -138.2274305149577\tEvaluator Episode Return: -149.41313297875615\n",
            "Episode: 700\tEpsilon: 0.05000000074505806\tEpisode Return: -137.64042395781829\tAverage Episode Return: -144.32458377758581\tEvaluator Episode Return: -197.55640557643403\n",
            "Episode: 710\tEpsilon: 0.05000000074505806\tEpisode Return: -132.68299713282704\tAverage Episode Return: -168.39693339619126\tEvaluator Episode Return: -134.45320784153074\n",
            "Episode: 720\tEpsilon: 0.05000000074505806\tEpisode Return: -143.6763377889773\tAverage Episode Return: -177.05963732638773\tEvaluator Episode Return: -143.92969229919342\n",
            "Episode: 730\tEpsilon: 0.05000000074505806\tEpisode Return: -152.07418446261497\tAverage Episode Return: -165.37208686054797\tEvaluator Episode Return: -128.45906326546725\n",
            "Episode: 740\tEpsilon: 0.05000000074505806\tEpisode Return: -137.42419967466182\tAverage Episode Return: -158.42128094538455\tEvaluator Episode Return: -146.86270475534462\n",
            "Episode: 750\tEpsilon: 0.05000000074505806\tEpisode Return: -131.0494278363627\tAverage Episode Return: -160.15767712498536\tEvaluator Episode Return: -165.49025016428797\n",
            "Episode: 760\tEpsilon: 0.05000000074505806\tEpisode Return: -96.23280455058705\tAverage Episode Return: -159.12745077593388\tEvaluator Episode Return: -158.4017175836489\n",
            "Episode: 770\tEpsilon: 0.05000000074505806\tEpisode Return: -172.06630854206952\tAverage Episode Return: -158.59777237485602\tEvaluator Episode Return: -183.2187307060194\n",
            "Episode: 780\tEpsilon: 0.05000000074505806\tEpisode Return: -156.9992561891505\tAverage Episode Return: -155.64885119480562\tEvaluator Episode Return: -140.09898920510753\n",
            "Episode: 790\tEpsilon: 0.05000000074505806\tEpisode Return: -79.0767654991237\tAverage Episode Return: -150.26441630289284\tEvaluator Episode Return: -154.77846915420562\n",
            "Episode: 800\tEpsilon: 0.05000000074505806\tEpisode Return: -184.03347463298672\tAverage Episode Return: -155.10785697620813\tEvaluator Episode Return: -145.36983289178852\n",
            "Episode: 810\tEpsilon: 0.05000000074505806\tEpisode Return: -165.90169963395815\tAverage Episode Return: -155.49874979180152\tEvaluator Episode Return: -138.54790229776765\n",
            "Episode: 820\tEpsilon: 0.05000000074505806\tEpisode Return: -78.51812341674082\tAverage Episode Return: -137.50156576088614\tEvaluator Episode Return: -145.5233716128294\n",
            "Episode: 830\tEpsilon: 0.05000000074505806\tEpisode Return: -62.1848339239847\tAverage Episode Return: -123.76054617590955\tEvaluator Episode Return: -147.86805973833543\n",
            "Episode: 840\tEpsilon: 0.05000000074505806\tEpisode Return: -89.65936989874656\tAverage Episode Return: -134.86929957537134\tEvaluator Episode Return: -142.1119822048728\n",
            "Episode: 850\tEpsilon: 0.05000000074505806\tEpisode Return: -69.12326639754878\tAverage Episode Return: -121.04896147082304\tEvaluator Episode Return: -106.84342464062928\n",
            "Episode: 860\tEpsilon: 0.05000000074505806\tEpisode Return: -75.1488579293685\tAverage Episode Return: -100.2566145250772\tEvaluator Episode Return: -134.6415658764798\n",
            "Episode: 870\tEpsilon: 0.05000000074505806\tEpisode Return: -253.93177283773838\tAverage Episode Return: -120.45636725359225\tEvaluator Episode Return: -162.08139032477658\n",
            "Episode: 880\tEpsilon: 0.05000000074505806\tEpisode Return: -223.8003787036347\tAverage Episode Return: -159.08729548359534\tEvaluator Episode Return: -169.75845872696985\n",
            "Episode: 890\tEpsilon: 0.05000000074505806\tEpisode Return: -114.82089070449854\tAverage Episode Return: -177.04182234522935\tEvaluator Episode Return: -166.80075990358063\n",
            "Episode: 900\tEpsilon: 0.05000000074505806\tEpisode Return: -181.33771770022003\tAverage Episode Return: -168.7299580740546\tEvaluator Episode Return: -168.06787700336005\n",
            "Episode: 910\tEpsilon: 0.05000000074505806\tEpisode Return: -143.11599236982545\tAverage Episode Return: -168.7788211807379\tEvaluator Episode Return: -154.1083923318363\n",
            "Episode: 920\tEpsilon: 0.05000000074505806\tEpisode Return: -189.2297896660421\tAverage Episode Return: -162.9475372529974\tEvaluator Episode Return: -161.3017901692118\n",
            "Episode: 930\tEpsilon: 0.05000000074505806\tEpisode Return: -167.28173645875523\tAverage Episode Return: -158.24283081390323\tEvaluator Episode Return: -157.54202049671088\n",
            "Episode: 940\tEpsilon: 0.05000000074505806\tEpisode Return: -158.78234500842782\tAverage Episode Return: -160.90932970854715\tEvaluator Episode Return: -154.02453856924012\n",
            "Episode: 950\tEpsilon: 0.05000000074505806\tEpisode Return: -193.3804020050921\tAverage Episode Return: -162.43651852065378\tEvaluator Episode Return: -152.15483712394715\n",
            "Episode: 960\tEpsilon: 0.05000000074505806\tEpisode Return: -146.63435975507664\tAverage Episode Return: -159.6227555982687\tEvaluator Episode Return: -166.2084847934143\n",
            "Episode: 970\tEpsilon: 0.05000000074505806\tEpisode Return: -131.76713385088757\tAverage Episode Return: -147.32996732046416\tEvaluator Episode Return: -149.7590620913598\n",
            "Episode: 980\tEpsilon: 0.05000000074505806\tEpisode Return: -156.1799763192687\tAverage Episode Return: -147.32770494661517\tEvaluator Episode Return: -153.22736458502416\n",
            "Episode: 990\tEpsilon: 0.05000000074505806\tEpisode Return: -155.32878868739465\tAverage Episode Return: -152.66552773176224\tEvaluator Episode Return: -152.90814195542717\n",
            "Episode: 1000\tEpsilon: 0.05000000074505806\tEpisode Return: -157.2974956115324\tAverage Episode Return: -157.28398587043816\tEvaluator Episode Return: -158.59957925895364\n",
            "Episode: 1010\tEpsilon: 0.05000000074505806\tEpisode Return: -171.02051808488244\tAverage Episode Return: -160.15303188107848\tEvaluator Episode Return: -154.35783667234585\n",
            "Episode: 1020\tEpsilon: 0.05000000074505806\tEpisode Return: -138.79817511002267\tAverage Episode Return: -155.39889493229197\tEvaluator Episode Return: -146.19537734414814\n",
            "Episode: 1030\tEpsilon: 0.05000000074505806\tEpisode Return: -158.1410089407968\tAverage Episode Return: -154.79549490853688\tEvaluator Episode Return: -157.56129691207633\n",
            "Episode: 1040\tEpsilon: 0.05000000074505806\tEpisode Return: -107.41830705564837\tAverage Episode Return: -151.51000839388738\tEvaluator Episode Return: -149.6375939570433\n",
            "Episode: 1050\tEpsilon: 0.05000000074505806\tEpisode Return: -182.38433948904435\tAverage Episode Return: -160.5748427971625\tEvaluator Episode Return: -160.8721416207189\n",
            "Episode: 1060\tEpsilon: 0.05000000074505806\tEpisode Return: -201.92024904278082\tAverage Episode Return: -173.98286914959922\tEvaluator Episode Return: -162.17649815969793\n",
            "Episode: 1070\tEpsilon: 0.05000000074505806\tEpisode Return: -202.04261480204775\tAverage Episode Return: -168.5356528443542\tEvaluator Episode Return: -176.86737993233976\n",
            "Episode: 1080\tEpsilon: 0.05000000074505806\tEpisode Return: -165.87475227564232\tAverage Episode Return: -163.20314216809865\tEvaluator Episode Return: -167.59963045566042\n",
            "Episode: 1090\tEpsilon: 0.05000000074505806\tEpisode Return: -218.17449429930866\tAverage Episode Return: -172.37356114020707\tEvaluator Episode Return: -195.10432558761198\n",
            "Episode: 1100\tEpsilon: 0.05000000074505806\tEpisode Return: -226.1152197630283\tAverage Episode Return: -199.0444372754329\tEvaluator Episode Return: -177.94862179957312\n",
            "Episode: 1110\tEpsilon: 0.05000000074505806\tEpisode Return: -299.6558552813965\tAverage Episode Return: -202.4026888450913\tEvaluator Episode Return: -211.96510065310733\n",
            "Episode: 1120\tEpsilon: 0.05000000074505806\tEpisode Return: -207.3623012609615\tAverage Episode Return: -201.8552596536364\tEvaluator Episode Return: -192.1880506140426\n",
            "Episode: 1130\tEpsilon: 0.05000000074505806\tEpisode Return: -208.3769379163379\tAverage Episode Return: -219.52349215045857\tEvaluator Episode Return: -233.49133088996518\n",
            "Episode: 1140\tEpsilon: 0.05000000074505806\tEpisode Return: -143.82310872421925\tAverage Episode Return: -212.25937813014602\tEvaluator Episode Return: -205.50613747432666\n",
            "Episode: 1150\tEpsilon: 0.05000000074505806\tEpisode Return: -249.6372421075415\tAverage Episode Return: -196.9942149528387\tEvaluator Episode Return: -216.10251760948066\n",
            "Episode: 1160\tEpsilon: 0.05000000074505806\tEpisode Return: -216.47026784340656\tAverage Episode Return: -179.43077390498914\tEvaluator Episode Return: -163.82112216645146\n",
            "Episode: 1170\tEpsilon: 0.05000000074505806\tEpisode Return: -174.02837637065159\tAverage Episode Return: -172.60907982130166\tEvaluator Episode Return: -182.78543300922496\n",
            "Episode: 1180\tEpsilon: 0.05000000074505806\tEpisode Return: -189.04970548500455\tAverage Episode Return: -167.48853921185\tEvaluator Episode Return: -147.76588024410376\n",
            "Episode: 1190\tEpsilon: 0.05000000074505806\tEpisode Return: -192.85982203234573\tAverage Episode Return: -160.6283145475324\tEvaluator Episode Return: -135.2387490934095\n",
            "Episode: 1200\tEpsilon: 0.05000000074505806\tEpisode Return: -159.71041906776105\tAverage Episode Return: -156.40445989607278\tEvaluator Episode Return: -134.74537096053683\n",
            "Episode: 1210\tEpsilon: 0.05000000074505806\tEpisode Return: -149.89143046405852\tAverage Episode Return: -138.72655476154094\tEvaluator Episode Return: -132.19113714951428\n",
            "Episode: 1220\tEpsilon: 0.05000000074505806\tEpisode Return: -79.17967108632925\tAverage Episode Return: -127.34368866926647\tEvaluator Episode Return: -120.43733387779055\n",
            "Episode: 1230\tEpsilon: 0.05000000074505806\tEpisode Return: -101.2960199634544\tAverage Episode Return: -126.5931217890477\tEvaluator Episode Return: -115.74067314894408\n",
            "Episode: 1240\tEpsilon: 0.05000000074505806\tEpisode Return: -102.9578248548432\tAverage Episode Return: -125.84634944790022\tEvaluator Episode Return: -112.26859690472162\n",
            "Episode: 1250\tEpsilon: 0.05000000074505806\tEpisode Return: -135.49774580250957\tAverage Episode Return: -126.53068308239403\tEvaluator Episode Return: -123.03097003960342\n",
            "Episode: 1260\tEpsilon: 0.05000000074505806\tEpisode Return: -141.9122245714125\tAverage Episode Return: -125.26269928012364\tEvaluator Episode Return: -117.712823514498\n",
            "Episode: 1270\tEpsilon: 0.05000000074505806\tEpisode Return: -150.06985665094993\tAverage Episode Return: -122.71126338533523\tEvaluator Episode Return: -135.31281824998842\n",
            "Episode: 1280\tEpsilon: 0.05000000074505806\tEpisode Return: -156.1365527252438\tAverage Episode Return: -117.17399736318676\tEvaluator Episode Return: -125.47254070766836\n"
          ]
        }
      ],
      "source": [
        "# Run environment loop\n",
        "print(\"Starting training. This may take a few minutes to complete.\")\n",
        "episode_returns, evaluator_returns = run_training_loop(\n",
        "                                        env_name,\n",
        "                                        Q_LEARNING_PARAMS,\n",
        "                                        q_learning_select_action_jit,\n",
        "                                        Q_LEARNING_ACTOR_STATE,\n",
        "                                        q_learn_jit,\n",
        "                                        Q_LEARNING_LEARN_STATE,\n",
        "                                        Q_LEARNING_MEMORY,\n",
        "                                        num_episodes=2000,\n",
        "                                        train_every_timestep=True, # do learning after every timestep\n",
        "                                        video_subdir=\"q_learning\"\n",
        "                                    )\n",
        "\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Episode Return\")\n",
        "plt.title(\"Deep Q-Learning\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k0-41wbpFDE"
      },
      "source": [
        "At this stage, the approximated Q-function hopefully converged to a decent policy for balancing the pole in the CartPole problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4v0ZbB4FQUs"
      },
      "outputs": [],
      "source": [
        "#@title Visualise Policy\n",
        "#@markdown Choose an episode number that is a multiple of 100 and less than or equal to 1000, and **run this cell**.\n",
        "\n",
        "episode_number = 900 #@param {type:\"number\"}\n",
        "\n",
        "assert (episode_number % 100) == 0, \"Episode number must be a multiple of 100 since we only record every 100th episode.\"\n",
        "assert episode_number < 1001, \"Episode number must be less than or equal to 1000\"\n",
        "\n",
        "eval_episode_number = int(episode_number / 100 * 80)\n",
        "video_path = f\"./video/q_learning/eval/rl-video-episode-{eval_episode_number}.mp4\"\n",
        "\n",
        "mp4 = open(video_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIIteLdFwG4B"
      },
      "source": [
        "For the original research paper where DQN was first introduced, see here: [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602). If you are interested, give the paper a read!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## **Now... to the moon!**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1194/1*Dj2fkRjrMA0w9E-PuyETdg.gif\" width=\"60%\" />\n",
        "</center>\n",
        "\n",
        "Once you have successfully solved CartPole using DQN, you should use what you have learned to help Steve land on the moon!\n",
        "\n",
        "For this you will use the [LunarLander](https://www.gymlibrary.ml/environments/box2d/lunar_lander/) environment. Simply go to the start of the notebook and replace the environment with LunarLander by replacing `env = gym.make(\"CartPole-v1\")` with env = `gym.make(\"LunarLander-v2\")`. Note, LunarLander is a significantly more challenging environment than CartPole. Therefore, you will likely need to experiment quite a bit to find the best hyperparameters for DQN, and possibly increase the number of training episodes in order to learn a good policy. In LunarLander, the environment is considered solved when the agent can reliably achieve an episode return of 200.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F_hhyvrRa9V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f35138c66f99be0fd7a1210bb4aa94a6fbaa5f29d51ad43aec0e0ea0ff050f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}